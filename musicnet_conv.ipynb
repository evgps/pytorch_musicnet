{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.1\n"
     ]
    }
   ],
   "source": [
    "import sys,os,errno,signal,copy\n",
    "from contextlib import contextmanager\n",
    "# !pip install intervaltree\n",
    "import numpy as np\n",
    "import musicnet\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.functional import conv1d, mse_loss, cross_entropy\n",
    "print(torch.__version__)\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "root = '/mnt/musicnet/'\n",
    "checkpoint_path = './checkpoints'\n",
    "checkpoint = 'musicnet_demo.pt'\n",
    "\n",
    "try:\n",
    "    os.makedirs(checkpoint_path)\n",
    "except OSError as e:\n",
    "    if e.errno != errno.EEXIST:\n",
    "        raise\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'   # see issue #152\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1,2,3'\n",
    "\n",
    "def worker_init(args):\n",
    "    signal.signal(signal.SIGINT, signal.SIG_IGN) # ignore signals so parent can handle them\n",
    "    np.random.seed(os.getpid() ^ int(time())) # approximately random seed for workers\n",
    "\n",
    "batch_size = 100\n",
    "kwargs = {'num_workers': 4, 'pin_memory': True, 'worker_init_fn': worker_init}\n",
    "\n",
    "m = 128\n",
    "k = 256\n",
    "d = 4096\n",
    "window = 16384\n",
    "stride = 512\n",
    "regions = 1 + (window - d)/stride\n",
    "# sig_len = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = musicnet.MusicNet(root=root, train=True, download=False, window=window)#, pitch_shift=5, jitter=.1)\n",
    "test_set = musicnet.MusicNet(root=root, train=False, window=window, epoch_size=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "naruto_test = musicnet.MusicNet(root=root, train=False, window=window, download=False, epoch_size=50000, naruto=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_set,batch_size=batch_size,**kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_set,batch_size=batch_size,**kwargs)\n",
    "naruto_loader = torch.utils.data.DataLoader(dataset=naruto_test,batch_size=batch_size,**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_filters(d,k,low=50,high=6000):\n",
    "    x = np.linspace(0, 2*np.pi, d, endpoint=False)\n",
    "    wsin = np.empty((k,1,d), dtype=np.float32)\n",
    "    wcos = np.empty((k,1,d), dtype=np.float32)\n",
    "    start_freq = low\n",
    "    end_freq = high\n",
    "    num_cycles = start_freq*d/44000.\n",
    "    scaling_ind = np.log(end_freq/start_freq)/k\n",
    "    window_mask = 1.0-1.0*np.cos(x)\n",
    "    for ind in range(k):\n",
    "        wsin[ind,0,:] = window_mask*np.sin(np.exp(ind*scaling_ind)*num_cycles*x)\n",
    "        wcos[ind,0,:] = window_mask*np.cos(np.exp(ind*scaling_ind)*num_cycles*x)\n",
    "    \n",
    "    return wsin,wcos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L(y_hat, y):\n",
    "    # adjust for per-frame loss\n",
    "    return mse_loss(y_hat, y)*128/2.\n",
    "\n",
    "def L(y_hat, y):\n",
    "    # adjust for per-frame loss\n",
    "    return cross_entropy(y_hat, y.type(torch.cuda.LongTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(torch.nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, avg=.9998):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        wsin,wcos = create_filters(d,k)\n",
    "        with torch.cuda.device(0):\n",
    "            self.wsin_var = Variable(torch.from_numpy(wsin).cuda(), requires_grad=False)\n",
    "            self.wcos_var = Variable(torch.from_numpy(wcos).cuda(), requires_grad=False)\n",
    "        h1 = 256\n",
    "        h2 = 256\n",
    "        self.seq = torch.nn.Sequential()\n",
    "        conv1_stride = 1\n",
    "        self.seq.add_module('drop', torch.nn.Dropout())\n",
    "#         self.seq.add_module('bn', torch.nn.BatchNorm1d(k))\n",
    "        \n",
    "        self.seq.add_module('conv1', torch.nn.Conv1d(1, 16, kernel_size = 16, padding=7, stride=16))\n",
    "        self.seq.add_module('conv2', torch.nn.Conv1d(16, 64, kernel_size = 16, padding=7, stride=16))\n",
    "#         self.seq.add_module('conv1', torch.nn.Conv1d(1, 64, kernel_size = 3, padding=1, stride=1))\n",
    "#         self.seq.add_module('conv1', torch.nn.Conv1d(1, 64, kernel_size = 3, padding=1, stride=1))\n",
    "\n",
    "        self.seq.add_module('flat', Flatten())\n",
    "#         self.seq.add_module('conv2', torch.nn.Conv1d(64, 16, kernel_size = 3, padding=1, stride=1))\n",
    "#         self.seq.add_module('conv3', torch.nn.Conv1d(16, 1, kernel_size = 3, padding=1, stride=1))\n",
    "        self.seq.add_module('lout', torch.nn.Linear(int(window/16/16*64), int(m), bias=False))\n",
    "        \n",
    "#         self.seq.add_module('l1', torch.nn.Linear(int(regions*k), int(h1), bias=False))\n",
    "#         self.seq.add_module('relu1', torch.nn.ELU())\n",
    "#         self.seq.add_module('l2', torch.nn.Linear(int(h1), int(h2), bias=False))\n",
    "#         self.seq.add_module('relu2', torch.nn.ELU())\n",
    "#         self.seq.add_module('l3', torch.nn.Linear(int(h2), int(m), bias=False))\n",
    "\n",
    "\n",
    "        self.seq.add_module('out', torch.nn.LogSoftmax())\n",
    "    \n",
    "    \n",
    "#         self.seq.add_module('out', torch.nn.LogSigmoid())\n",
    "#         self.seq.add_module('out', torch.nn.Sigmoid())\n",
    "#         self.linear = torch.nn.Linear(int(regions*k), int(h), bias=False).cuda()\n",
    "#         torch.nn.init.xavier_uniform(self.linear.weight)\n",
    "#         self.linear2 = torch.nn.Linear(int(h), int(h), bias=False).cuda()\n",
    "#         torch.nn.init.xavier_uniform(self.linear2.weight)\n",
    "#         self.linear3 = torch.nn.Linear(int(h), int(m), bias=False).cuda()\n",
    "#         torch.nn.init.xavier_uniform(self.linear3.weight)\n",
    "        self.seq.cuda()\n",
    "        self.avg = avg\n",
    "        self.averages = copy.deepcopy(list(parm.data for parm in self.parameters()))\n",
    "        for (name,parm),pavg in zip(self.named_parameters(),self.averages):\n",
    "            self.register_buffer(name + '.avg', pavg)\n",
    "    \n",
    "    def forward(self, x):\n",
    "#         print(x.size())\n",
    "        zx = conv1d(x[:,None,:], self.wsin_var, stride=stride).pow(2) \\\n",
    "           + conv1d(x[:,None,:], self.wcos_var, stride=stride).pow(2)\n",
    "#         print(zx.size())\n",
    "#         hid = self.linear(torch.log(zx + musicnet.epsilon).view(x.data.size()[0],int(regions*k)))\n",
    "#         hid2 = self.linear2(F.relu(hid))\n",
    "#         hid3 = self.linear3(F.relu(hid2))\n",
    "#         return F.softmax(hid3)\n",
    "#         return self.seq.forward(torch.log(zx + musicnet.epsilon).view(x.data.size()[0],int(regions*k)))\n",
    "#         return self.seq.forward(torch.log(zx + musicnet.epsilon))print\n",
    "        return self.seq.forward(x.view(x.data.size()[0],1,int(window)))\n",
    "\n",
    "    \n",
    "    def average_iterates(self):\n",
    "        for parm, pavg in zip(self.parameters(), self.averages):\n",
    "            pavg.mul_(self.avg).add_(1.-self.avg, parm.data)\n",
    "\n",
    "@contextmanager\n",
    "def averages(model):\n",
    "    orig_parms = copy.deepcopy(list(parm.data for parm in model.parameters()))\n",
    "    for parm, pavg in zip(model.parameters(), model.averages):\n",
    "        parm.data.copy_(pavg)\n",
    "    yield\n",
    "    for parm, orig in zip(model.parameters(), orig_parms):\n",
    "        parm.data.copy_(orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (seq): Sequential(\n",
      "    (drop): Dropout(p=0.5)\n",
      "    (conv1): Conv1d(1, 16, kernel_size=(16,), stride=(16,), padding=(7,))\n",
      "    (conv2): Conv1d(16, 64, kernel_size=(16,), stride=(16,), padding=(7,))\n",
      "    (flat): Flatten(\n",
      "    )\n",
      "    (lout): Linear(in_features=4096, out_features=128, bias=False)\n",
      "    (out): LogSoftmax()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "print (model)\n",
    "loss_history = []\n",
    "avgp_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (optional) restore from checkpoint, if it exists\n",
    "# try:\n",
    "#     model.load_state_dict(torch.load(os.path.join(checkpoint_path,checkpoint)))\n",
    "# except IOError as e:\n",
    "#     if e.errno != errno.ENOENT:\n",
    "#         raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "square loss\tavg prec\ttime\t\tutime\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/nn/modules/container.py:67: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.089055\t0.058829\t28.694576\t8.882244\n",
      "0.083846\t0.064142\t27.502878\t9.146464\n",
      "0.080853\t0.066408\t26.665220\t9.206639\n",
      "0.078249\t0.066653\t25.998415\t9.584826\n",
      "0.077299\t0.066661\t25.144927\t9.498252\n",
      "0.077217\t0.067249\t26.190445\t9.583795\n",
      "0.076527\t0.067736\t25.122895\t9.943019\n",
      "0.076456\t0.067106\t25.297190\t10.193533\n",
      "0.076178\t0.066996\t25.038112\t9.296016\n",
      "0.076028\t0.067802\t25.877631\t8.682629\n",
      "0.076054\t0.066966\t27.029356\t9.661135\n",
      "0.076183\t0.067389\t27.316587\t9.748882\n",
      "0.076029\t0.067684\t27.500017\t9.950887\n",
      "0.076082\t0.068249\t27.004697\t9.442557\n",
      "0.076331\t0.067837\t28.146962\t11.995288\n",
      "0.076631\t0.068431\t28.311491\t10.973042\n",
      "0.076420\t0.068364\t25.732841\t9.581273\n",
      "0.076318\t0.068023\t25.655419\t9.768300\n",
      "0.075830\t0.067691\t25.331743\t9.688355\n",
      "0.076058\t0.068403\t25.584362\t10.205876\n",
      "0.075961\t0.068765\t25.606829\t9.786461\n",
      "0.076408\t0.068780\t25.482717\t9.685436\n",
      "0.076178\t0.067741\t22.233274\t9.080993\n",
      "0.076104\t0.068731\t26.012631\t9.297248\n",
      "0.076316\t0.069025\t26.101165\t8.692589\n",
      "0.076220\t0.068548\t26.277414\t9.029017\n",
      "0.076016\t0.069684\t23.243008\t8.233251\n",
      "0.076112\t0.069845\t21.556682\t8.468074\n",
      "0.075942\t0.068934\t26.310144\t9.742981\n",
      "0.075947\t0.068826\t26.782512\t9.362060\n",
      "0.076142\t0.068605\t26.625167\t10.584926\n",
      "0.075940\t0.067958\t26.225248\t9.339361\n",
      "0.076037\t0.067649\t25.468035\t9.644441\n",
      "0.076052\t0.067774\t24.429820\t10.036577\n",
      "0.075980\t0.068428\t24.541044\t9.255425\n",
      "0.076129\t0.068854\t25.554094\t9.349877\n",
      "0.076098\t0.069357\t25.557732\t9.004011\n",
      "0.075945\t0.068799\t25.890102\t9.117249\n",
      "0.075775\t0.069211\t26.138448\t8.813492\n",
      "0.076039\t0.069792\t25.080217\t8.046264\n",
      "0.075757\t0.069101\t20.930797\t8.128006\n",
      "0.075991\t0.069365\t22.369517\t9.016216\n",
      "0.076186\t0.068648\t26.107920\t9.155570\n",
      "0.076363\t0.068544\t26.897681\t9.460978\n",
      "0.075527\t0.067325\t27.361318\t9.336177\n",
      "0.075960\t0.068622\t27.251483\t9.955872\n",
      "0.076097\t0.068462\t27.849470\t9.973367\n",
      "0.075987\t0.069408\t26.402429\t9.409686\n",
      "0.075870\t0.068454\t26.414648\t9.856155\n",
      "0.076181\t0.069528\t25.218300\t9.425783\n",
      "0.075920\t0.068899\t25.567230\t9.379453\n",
      "0.075982\t0.069066\t24.809260\t9.656622\n"
     ]
    }
   ],
   "source": [
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, momentum=.95)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = torch.nn.MultiLabelSoftMarginLoss()\n",
    "try:\n",
    "    with train_set, test_set:\n",
    "        print ('square loss\\tavg prec\\ttime\\t\\tutime')\n",
    "        for epoch in range(500):\n",
    "            t = time()\n",
    "            for i, (x, y) in enumerate(train_loader):\n",
    "                x, y = Variable(x.cuda(), requires_grad=True), Variable(y.cuda(), requires_grad=False)\n",
    "#                 print(y.mean())\n",
    "                #                 loss = L(model(x),y)\n",
    "                loss = criterion(model(x),y)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                model.average_iterates()\n",
    "\n",
    "            t1 = time()\n",
    "            avgp, loss = 0., 0.\n",
    "            yground = torch.FloatTensor(batch_size*len(test_loader), m)\n",
    "            yhat = torch.FloatTensor(batch_size*len(test_loader), m)\n",
    "            with averages(model):\n",
    "                for i, (x, y) in enumerate(test_loader):\n",
    "                    x, y = Variable(x.cuda(), requires_grad=False), Variable(y.cuda(), requires_grad=False)\n",
    "                    yhatvar = model(x)\n",
    "#                     loss += L(yhatvar,y).data[0]\n",
    "                    loss += criterion(yhatvar,y).data[0]\n",
    "                    yground[i*batch_size:(i+1)*batch_size,:] = y.data\n",
    "                    yhat[i*batch_size:(i+1)*batch_size,:] = yhatvar.data\n",
    "            avgp = average_precision_score(yground.numpy().flatten(),yhat.numpy().flatten())\n",
    "            loss_history.append(loss/len(test_loader))\n",
    "            avgp_history.append(avgp)\n",
    "            torch.save(model.state_dict(), os.path.join(checkpoint_path,checkpoint))\n",
    "            print ('{:2f}\\t{:2f}\\t{:2f}\\t{:2f}'.format(loss_history[-1],avgp_history[-1],time()-t, time()-t1))\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print ('Graceful Exit')\n",
    "else:\n",
    "    print ('Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch_size*len(train_loader))\n",
    "print(batch_size*len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burnin = 1\n",
    "\n",
    "fig = plt.figure(figsize=(15, 7))\n",
    "fig.add_axes()\n",
    "\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax2 = fig.add_subplot(132)\n",
    "\n",
    "for ax in [ax1, ax2]:\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"left\"].set_visible(False)\n",
    "    ax.spines[\"bottom\"].set_visible(False)\n",
    "    ax.grid(color='b', linestyle='--', linewidth=0.5, alpha=0.3)\n",
    "    ax.tick_params(direction='out', color='b', width='2')\n",
    "    \n",
    "ax1.set_title('square loss')\n",
    "ax1.plot(np.arange(len(loss_history[burnin:])), loss_history[burnin:])\n",
    "ax2.set_title('average precision')\n",
    "ax2.plot(np.arange(len(avgp_history[burnin:])), avgp_history[burnin:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Реал тест на наруте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    with train_set, test_set:\n",
    "        print ('square loss\\tavg prec\\ttime\\t\\tutime')\n",
    "        for epoch in range(50):\n",
    "            t = time()\n",
    "            for x, y in train_loader:\n",
    "                x, y = Variable(x.cuda(), requires_grad=False), Variable(y.cuda(), requires_grad=False)\n",
    "                loss = L(model(x),y)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                model.average_iterates()\n",
    "\n",
    "            t1 = time()\n",
    "            avgp, loss = 0., 0.\n",
    "            yground = torch.FloatTensor(batch_size*len(test_loader), m)\n",
    "            yhat = torch.FloatTensor(batch_size*len(test_loader), m)\n",
    "            with averages(model):\n",
    "                for i, (x, y) in enumerate(naruto_loader):\n",
    "                    x, y = Variable(x.cuda(), requires_grad=False), Variable(y.cuda(), requires_grad=False)\n",
    "                    yhatvar = model(x)\n",
    "                    print(yhatvar)\n",
    "                    loss += L(yhatvar,y).data[0]\n",
    "                    yground[i*batch_size:(i+1)*batch_size,:] = y.data\n",
    "                    yhat[i*batch_size:(i+1)*batch_size,:] = yhatvar.data\n",
    "            avgp = average_precision_score(yground.numpy().flatten(),yhat.numpy().flatten())\n",
    "            loss_history.append(loss/len(test_loader))\n",
    "            avgp_history.append(avgp)\n",
    "            torch.save(model.state_dict(), os.path.join(checkpoint_path,checkpoint))\n",
    "            print ('{:2f}\\t{:2f}\\t{:2f}\\t{:2f}'.format(loss_history[-1],avgp_history[-1],time()-t, time()-t1))\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print ('Graceful Exit')\n",
    "else:\n",
    "    print ('Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
